import streamlit as st
import os
from dotenv import load_dotenv
from rag_integrado import RAGIntegrado

load_dotenv()

st.set_page_config(page_title="ğŸ§¬ RAG HÃ­brido", page_icon="ğŸ§¬", layout="wide")

if 'rag_system' not in st.session_state:
    st.session_state.rag_system = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

st.title("ğŸ§¬ RAG HÃ­brido: BM25 + FAISS")
st.markdown("*Fusionando la precisiÃ³n de bÃºsqueda por keywords con embeddings multilingÃ¼es*")

with st.sidebar:
    st.header("âš™ï¸ ConfiguraciÃ³n")
    
    api_key = os.getenv("GROQ_API_KEY", "")
    if not api_key:
        api_key = st.text_input("Introduce tu Groq API Key", type="password")
    
    uploaded_file = st.file_uploader("ğŸ“„ Subir documento (PDF)", type=['pdf'])
    
    st.subheader("ConfiguraciÃ³n de Chunks")
    chunk_size = st.slider("TamaÃ±o del chunk", 200, 1000, 500, 25)
    chunk_overlap = st.slider("SuperposiciÃ³n", 0, 300, 100, 25)
    
    st.subheader("Nivel de AnÃ¡lisis")
    search_mode = st.radio("Seleccionar modo:", 
                          ["BÃ¡sico (Solo FAISS)", "Mejorado (HÃ­brido BM25+FAISS + Multi-hop)"],
                          help="**BÃ¡sico**: BÃºsqueda vectorial simple + prompt estÃ¡ndar\n**Mejorado**: BÃºsqueda hÃ­brida + expansiÃ³n de conceptos + prompt forense")
    
    if st.button("ğŸš€ Procesar Documento"):
        if not api_key:
            st.error("âŒ Por favor configura tu API Key en el archivo .env o ingrÃ©sala arriba")
        elif not uploaded_file:
            st.error("âŒ Por favor sube un documento PDF")
        else:
            with st.spinner("ğŸ”„ Indexando con BM25 y FAISS..."):
                try:
                    # Inicializar sistema RAG integrado
                    st.session_state.rag_system = RAGIntegrado(
                        api_key=api_key,
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    
                    num_chunks = st.session_state.rag_system.process_document(uploaded_file)
                    
                    st.success(f"âœ… Documento procesado exitosamente!")
                    st.info(f"ğŸ“Š {num_chunks} fragmentos indexados con bÃºsqueda hÃ­brida")
                    st.session_state.chat_history = []
                except Exception as e:
                    st.error(f"âŒ Error al procesar: {str(e)}")
    
    if st.button("ğŸ—‘ï¸ Limpiar Chat"):
        st.session_state.chat_history = []
        st.rerun()

st.divider()
st.header("ğŸ’¬ Chat Forense")

for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Haz tu consulta forense aquÃ­..."):
    if st.session_state.rag_system is None:
        st.warning("âš ï¸ Por favor procesa un documento primero")
    else:
        # Agregar pregunta del usuario
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generar respuesta
        mode_key = "advanced" if "Mejorado" in search_mode else "basic"
        
        with st.chat_message("assistant"):
            with st.spinner("ğŸ•µï¸ Analizando documento..."):
                try:
                    response = st.session_state.rag_system.get_answer(prompt, mode=mode_key)
                    st.markdown(response)
                    st.session_state.chat_history.append({"role": "assistant", "content": response})
                except Exception as e:
                    st.error(f"âŒ Error al generar respuesta: {str(e)}")
